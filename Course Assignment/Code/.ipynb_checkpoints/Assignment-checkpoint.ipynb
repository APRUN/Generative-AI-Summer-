{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bb67f6-387b-443f-8076-3c9c11ca868d",
   "metadata": {},
   "source": [
    "# Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1c06f7-8a72-406f-bf61-ff5cf9607891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IATA_CODE                       AIRLINE\n",
      "0         UA         United Air Lines Inc.\n",
      "1         AA        American Airlines Inc.\n",
      "2         US               US Airways Inc.\n",
      "3         F9        Frontier Airlines Inc.\n",
      "4         B6               JetBlue Airways\n",
      "5         OO         Skywest Airlines Inc.\n",
      "6         AS          Alaska Airlines Inc.\n",
      "7         NK              Spirit Air Lines\n",
      "8         WN        Southwest Airlines Co.\n",
      "9         DL          Delta Air Lines Inc.\n",
      "10        EV   Atlantic Southeast Airlines\n",
      "11        HA        Hawaiian Airlines Inc.\n",
      "12        MQ  American Eagle Airlines Inc.\n",
      "13        VX                Virgin America\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the CSV file\n",
    "# Assuming the CSV has a column named 'text'\n",
    "df = pd.read_csv('airlines.csv')\n",
    "print(df.head(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666be7e6-1cd2-4a13-a0d8-fc795b757c80",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58473353-a6cb-4669-b635-6a51aec17cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\Chief\n",
      "[nltk_data]     Oggy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Chief\n",
      "[nltk_data]     Oggy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Chief\n",
      "[nltk_data]     Oggy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Chief\n",
      "[nltk_data]     Oggy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Choose a text corpus, for example, \"Alice in Wonderland\" by Lewis Carroll\n",
    "text = gutenberg.raw('carroll-alice.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e4426-aa19-4f60-8789-b6742d0d00ee",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c43bef-f8ec-4e08-b4ee-22a661a27a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 1625\n",
      "First 5 sentences: [\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\", \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\", 'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.', \"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\", 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Number of sentences:\", len(sentences))\n",
    "print(\"First 5 sentences:\", sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95154ead-8800-4d4f-b2bf-d420ca5301d6",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb09740-3647-4d12-92ad-a3dcb614724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 33494\n",
      "First 20 words: ['[', 'Alice', \"'s\", 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']', 'CHAPTER', 'I', '.', 'Down', 'the', 'Rabbit-Hole', 'Alice', 'was', 'beginning']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"First 20 words:\", words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9273338-f7b0-4dfc-b29f-de35faf23ae6",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35cf01c-dc62-44c5-8b0b-eec45d5b34d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 stemmed words: ['[', 'alic', \"'s\", 'adventur', 'in', 'wonderland', 'by', 'lewi', 'carrol', '1865', ']', 'chapter', 'i', '.', 'down', 'the', 'rabbit-hol', 'alic', 'wa', 'begin']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Apply stemming to the words\n",
    "stemmed_words = [ps.stem(word) for word in words]\n",
    "print(\"First 20 stemmed words:\", stemmed_words[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997786d-6f2d-4a98-b509-be45068aea8d",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f5ad0c3-9eb0-4291-b38c-74d0ed730c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 lemmatized words: ['[', 'Alice', \"'s\", 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']', 'CHAPTER', 'I', '.', 'Down', 'the', 'Rabbit-Hole', 'Alice', 'wa', 'beginning']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to the words\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"First 20 lemmatized words:\", lemmatized_words[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344f701-311e-4b03-98ab-05a3c7c7596e",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089dd36-64eb-40cf-8098-ea10bb70f624",
   "metadata": {},
   "source": [
    "**Impact of Tokenization:**\n",
    "\n",
    "1. Sentence tokenization helps in breaking down the text into manageable units for further processing.\n",
    "2. Word tokenization is essential for tasks like frequency analysis, stemming, and lemmatization.\n",
    "\n",
    "**Impact of Stemming:**\n",
    "1. Stemming can help reduce vocabulary size and normalize words to their base forms.\n",
    "2. It can be aggressive and sometimes produce non-dictionary words, which might lose some meaning.\n",
    "\n",
    "**Impact of Lemmatization:**\n",
    "1. Lemmatization results in meaningful base forms of words, improving text quality for analysis.\n",
    "2. It preserves the context and reduces the chances of creating non-dictionary words compared to stemming.\n",
    "\n",
    "**Impact of Stop Word Removal:**\n",
    "Removing stop words reduces noise in the text and focuses on the meaningful words.\n",
    "It helps in improving the efficiency and effectiveness of text analysis tasks such as text classification and topic modeling.\n",
    "\n",
    "**Each preprocessing step has a distinct impact on the text corpus:**\n",
    "\n",
    "**Tokenization:** Breaks text into smaller units (sentences and words), making it easier to process and analyze.\n",
    "**Stemming:** Reduces words to their root forms, helping in normalizing text but sometimes creating non-words.\n",
    "**Lemmatization:** Provides meaningful base forms by considering context, improving over stemming.\n",
    "**Stop Word Removal:** Eliminates common, less meaningful words, enhancing focus on significant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e73602-484b-4c66-bc47-8bcdc4f6a1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
